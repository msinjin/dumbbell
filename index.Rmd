---
title: "Human Activity Recognition: Dumbbell technique"
author: "Mark St. John"
date: '2018-09-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

People regularly quantify how much of a particular activity they do with wearable devices, but they rarely quantify how well they do it. This project seeks to determine the potential for real-time user feedback from model-based assessment of the quality, not quantity, of exercise technique.

To this end, six participants were asked to perform dumbbell lifts correctly and incorrectly in five different ways while accelerometers on the belt, forearm, arm, and dumbbell recorded their movements.

More information is available [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).

## Data 

The data for this project kindly provided by [Groupware@LES](http://groupware.les.inf.puc-rio.br), and can be downloaded from their website ([training data (csv, 12MB)](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), [test data (csv, 15 KB)](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)). A considerable amount of cleaning was needed, as described next, prior to analysis.

```{r data_cleaning, message = F, warning = F}

library(ggplot2)
library(caret)
library(plyr)
set.seed(42)
# Read in the data, suppressing the assignment of factors and dealing with missing values.
training <- read.csv("pml-training.csv", 
                     stringsAsFactors = F, 
                     na.strings = c("#DIV/0!","","NA"))
testing <- read.csv("pml-testing.csv", 
                    stringsAsFactors = F, 
                    na.strings = c("#DIV/0!","","NA"))

clean_data <- function(d){

# The data contain several unneccesary, non-sensor variables/features that need to be dropped. One problematic variable, "new_window", has levels "yes"/"no" in the training data, and only "no" in the testing data. "new_window", therefore, is likely a data flag the original researchers included to specify a subset of data for a purpose that is unknown (although it is likely these are summary statistics). Regardless, since "new_window" is a valid feature of only one dataset, all rows where "new_window" = "yes" will be removed and the feature dropped.
    
    d <- d[d$new_window == "no",]
    drop_vars <- c("X", 
                   "raw_timestamp_part_1", 
                   "raw_timestamp_part_2", 
                   "cvtd_timestamp", 
                   "new_window", 
                   "num_window")
    keep_vars <- !names(d) %in% drop_vars
    d <- d[, keep_vars]
    
    # Properly code factors:
    all_possible_factor_vars <- c("user_name", "classe", "problem_id")
    factor_vars <- names(d)[names(d) %in% all_possible_factor_vars]
    for(i in factor_vars){
        d[,i] <- as.factor(d[,i])
    }
    
    # Fix character fields which should be numeric:
    chr2num_vars <- names(d)[unlist(lapply(d, function(x) is.character(x)))]
    for(i in chr2num_vars){
        d[,i] <- as.numeric(d[,i]) 
    }
    
    # Fix logical fields which should be numeric:
    logical2num_vars <- names(d)[unlist(lapply(d, function(x) is.logical(x)))]
    for(i in logical2num_vars){
        d[,i] <- as.numeric(d[,i]) 
    }
    d
}

train_clean <- clean_data(training)
test_clean <- clean_data(testing)

# Some features/variables have no data in the test set. Furthermore, their names incicate that they are summary statistics (max_, min_, avg_, var_, etc.). Thus variables in the training set we limited to those in the test set.

no_test_data_var_names <- names(test_clean)[
    unlist(lapply(test_clean, function(x) 
        sum(is.na(x)))) == nrow(test_clean)
    ]

valid_test_names <- names(test_clean)[
    !names(test_clean) %in% no_test_data_var_names
    ]

train_valid <- train_clean[,names(train_clean) %in% c(valid_test_names, "classe")]
test_valid <- test_clean[, names(test_clean) %in% valid_test_names]

# The provided "testing" dataset does not acutally have the variable we are trying to predict ("classe") and so it is impossible to use it to test our model. Thus training, validation and testing datasets were created from the provided training dataset and reserve the provided test dataset for the final prediction exercise.

test_final <- test_valid

# A complication to creating our data partitions is that there is clear evidence for dependence between predictor variables and test subjects. As well, there are an unequal number of observations of each test subject. For example, observe how half the subjects have "roll_belt" values consistently about zero while the other half are about 125-150.

ggplot(train_valid, aes(x = seq_along(row.names(train_valid)), y = roll_belt, col = user_name)) +
    geom_jitter(shape = 1)

# Thus, to avoid the chance of unintended bias data partitions were created by individual and then recombined. This guaranteed no user bias accross data partitions. Granted, a simple random reshuffling of the dataset prior to creating partitions would likely produce similar results.

list_of_data_partitions <- dlply(train_valid, "user_name", function(x){

            inBuild <- createDataPartition(x[,"classe"], 
                                           p = 0.7)[[1]]
            build_df <- x[inBuild,]
            validation_df <- x[-inBuild,]
            inTrain <- createDataPartition(build_df[,"classe"], 
                                           p = 0.7)[[1]]
            train_df <- build_df[inTrain,]
            test_df <- build_df[-inTrain,]
            list(train_df = train_df, 
                 validation_df = validation_df,
                 test_df = test_df)
        }
    )

train_df <- ldply(list_of_data_partitions, function(x) x[["train_df"]])
validation_df <- ldply(list_of_data_partitions, function(x) x[["validation_df"]])
test_df <- ldply(list_of_data_partitions, function(x) x[["test_df"]])

# Confirm it all went to plan (Note, average mean relative differences approached 5% when partitioning data irrespective of subjects):
c(all.equal(table(train_df$user_name)/nrow(train_df),
          table(test_df$user_name)/nrow(test_df),
          tolerance = 0.01),
all.equal(table(train_df$user_name)/nrow(train_df),
          table(validation_df$user_name)/nrow(validation_df),
          tolerance = 0.01))

# Remove user_name now that it is no longer needed
remove_user <- function(x,var){
    x[, !names(x) %in% var]
    }
train_df <- remove_user(train_df,"user_name")
test_df <- remove_user(test_df,"user_name")
validation_df <- remove_user(validation_df,"user_name")
test_final <- remove_user(test_final,"user_name")

# Clean up environment
rm(list = ls()[!ls() %in% c("test_final", "train_df", "test_df", "validation_df")])
```

# Building the model

Different classes of models were considered as candidates to predict the classification of dumbbell lifts. Bagged (random forest, `rf`) and boosted (gradient boosted model, `gbm`) models were assumed to be the best approaches given a classification problem with no need for model interpretability. These models also perform well with the type of data provided (a massive number of predictors, likely high in collinearity, interactions and thresholds), relative to regression techniques. However, regression (generalized linear model via penalized maximum likelihood, `glmnet`) and model-based prediction methods (linear discriminant analysis, `lda`) were considered for comparison and have the advantage of usually being faster (although this was not an important factor in choosing a model).

## Cross validation

Repeated cross validation (10 folds, repeated 5 times) was chosen to estimate out of sample error using the training data as this gives an excellent balance of bias and variance. A large value of the tuning parameter (`tuneLength = 10`) was used to allow more predictors to be assessed at each split in the `rf` model. This increased runtime significantly, but testing demonstrated that it did produce lower estimates of out of sample error. Pre-processing the data was considered (centre, scale and PCA), but performed worse on all but regression models and so was only used for `glmnet`.

```{r cross_validation, message = F, cache = T, results = 'hide'}

# Set up for all models ----
set.seed(321)
# nearZeroVar(train_df) # no vars with zero variance, continue.
# Use same training controls and CV folds from all models:
myFolds <- createFolds(train_df$classe, 
                       k = 10, 
                       list = T, 
                       returnTrain = T) 
trControl <- trainControl(method = "repeatedcv", 
                          classProbs = T, 
                          repeats = 5, 
                          index = myFolds, 
                          verboseIter = F, 
                          savePredictions = T)

# Run all models ----
# Using caretEnsenble to unify options,  caretList() creates a list of train() objects.
library(caretEnsemble)
library(ranger) # alternative to base rf model
model_list <- caretList(classe ~ .,
                        data = train_df,
                        methodList = c(lda = "lda", 
                                       gbm = "gbm", 
                                       rf = "ranger"),
                        tuneList = list(glmnet = caretModelSpec("glmnet",
                                                                preProcess = c("center",
                                                                               "scale"))),
                        trControl = trControl,
                        tuneLength = 10)

# Compare model performance with a plot
resamps <- resamples(model_list)
dotplot(resamps)
```

### Expected out of sample error

Based on the dotplot, it appears the `rf` model had a slight advantage with a predicted out of sample error rate of less than 3%. 

```{r oob_rf}
summary(resamps)
model_list$rf$finalModel
```
A comparison of predictions for both models on the validation test set (not shown) confirms this and that the `gbm` model was also a condidate.

Leading us to conclude that the `rf` model was the most accurate. Finally, performace of the `rf model was assessed on the test set where is was confirmed to perform well.

```{r test_sample_error}
confusionMatrix(predict(model_list$rf,test_df),test_df$classe)
```

# Conclusions

Our random forest (`rf`) model allowed the prediction of dumbbell lift classification with just over 99% accuracy. The approach to using wearable computing devices to determine the quality of exercise appears to have merit.
